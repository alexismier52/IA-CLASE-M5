{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correos electronicos SPAM: Un enfoque con Procesamiento de Lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diego Alexis Mier Rutiaga No. 16111728"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los correos electronicos no deseados en su bandeja de entrada son molestos ya que perturban la rutina del usuario. es por eso que las cuentas de correo electronico ya tiene un filtro de spam. Daddo que es una de las aplicaciones del PLN mas utilizadas vamos a ver como se desarrollo un filtro de spam simple para correos electronicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos la famosas librerias\n",
    "from functools import reduce\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insertando los datos\n",
    "full_corpus = pd.read_csv('SMSSpamCollection.tsv', sep= '\\t', header=None, names=['label', 'msg_body'])\n",
    "\n",
    "#Separando los mensajes en ham y spam\n",
    "ham_text = []\n",
    "spam_text= []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los N-Gramos se usan para modelar el Lenguaje en funcion de la prediccion de palabras, es decir, predice la siguiente palabra de una oracion de palabras N-1 anteriores. Bigram es la secuencia de 2 palabras de N- gramos que predice la siguiente palabra de una oracion usando la palbra anterior. En lugar de considerar la historia completa de una oracion o una secuencia de palabras en particular, un modelo como bigram puede ser ocupado en terminos de una aproximacion de la historia al ocupar una historia limitada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La identificacion de un mensaje como \"ham\" o \"spam\" es una tarea de clasificacion ya que la variable de destino tiene valores discretos que son ham o spam. En esta practica, se usa el modelo bigram, aunque existen muchas tecnicas avanzadas que se pueden utilizar para este proposito. Para utilizar el modelo bigram para asignar un mensaje como spam o ham hay varios pasos que deben lograrse:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Inspeccion y separacion de mensajes en las categorias \"ham\" y \"spam\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente, el conjunto de datos debe inspeccionarse para ocuparlo y abordarlo para lograr la tarea. El formato de los datos dados, la cantidad de datos proporcionados, la naturaleza de los datos se incluyen en esta inspeccion para identificar la mejor aproximacion posible para la tarea "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El corpus de mensajes dado ha maracado cada mensaje como ham o spam. Ademas, hay 5568 mensajes en un DataFrame escrito en ingles que no son objetos nulos. Por lo tanto, el archivo tsv se puede leer usando DataFrame en python para clasificar esos mensajes de acuerdo con el indicador dado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_msgs():\n",
    "    for index, column in full_corpus.iterrows():\n",
    "        label= column[0]\n",
    "        message_text= column[1]\n",
    "        if label == 'ham':\n",
    "            ham_text.append(message_text)\n",
    "        elif label == 'spam':\n",
    "            spam_text.append(message_text)\n",
    "            \n",
    "separate_msgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento de texto "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El preprocesamiento es la tarea de realizar los pasos de preparaacion en el corpus de textos sin formato para completar de manera eficiente una extraccion de texto o procesamiento de lenguaje natural natural otra tarea que incluya texto sin formato. El preprocesamiento de texto consta de varios pasos, aunque algunos de ellos pueden no aplicarse a una tarea en particular debido a la naturaleza del conjunto de datos disponible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta tarea, el preprocesamiento de texto incluye los siguientes pasos de acuerdo con el conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminacion de signos de puntuacion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminacion de los signos de puntuacion de los mensajes de correo electronico\n",
    "def remove_msg_punctuations(email_msg):\n",
    "    punctuation_removed_msg = \"\". join([word for word in email_msg if word not in string.punctuation])\n",
    "    return punctuation_removed_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir a minusculas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir a minusculas: La conversion de todos los caracteres del texto en un contexto comun, como los soportes en minusculas, impide identificar dos palabras de manera diferente donde una esta en minuscula y la otra no. Por ejemplo, \"Primero\" y \"primero\" deben identificarse como iguales, por lo tanto, poner en minusculas todos los caracteresfacilita la tarea. Ademas, las palabras de detencion tambien estan en minusculas, por lo que esto tambien haria posible eliminar palabras de detencion mas adelante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing: La tokenizacion es la tarea de dividir el texto en partes significativas, es decir, tokens que incluyen oraciones y palabras. Un token se puede considerar como una instancia de una secuencia de caracteres en un texto particular que se agrupan para proporcionar una unidad semantica util para su posterior procesamiento. En esta tarea, la tokenizacion de palabras se realiza combinando espacios en blanco entre palabras como delimitador. Esto se logra en Python usando expresiones regulares para dividir una cadena en subcadenas con la funcion split(). que es un tokenizador basico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convierte el texto en minusculas y tokenizing de palabras\n",
    "def tokenize_into_words(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras lematizantes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La derivacion es el proceso de eliminar afijos (sufijos, prefijos, infijos, circunfijos) de una palabra para obtener su raiz de palabra. Aunque la lematizacion esta relacionada con la derivacion, difiere ya que la lematizacion puede capturar formas canonicas basadas en el tema de una palabra. La lematizacion ocupa un vocabulario y un analisis morfologico de las palabras que lo hacen mas rapido y preciso que la derivacion. WordNetLemmatizer ha logrado la lemmatizacion en lenguaje Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatization(tokenized_words):\n",
    "    lemmatized_text= [word_lemmatizer.lemmatize(word) for word in tokenized_words]\n",
    "    return ' '.join(lemmatized_text)\n",
    "\n",
    "def preprocessing_msgs(corpus):\n",
    "    categorized_text = pd.DataFrame(corpus)\n",
    "    categorized_text['non_punc_message_body'] = categorized_text[0].apply(lambda msg: remove_msg_puntuations(msg))\n",
    "    categorized_text['tokenized_msg_body'] = categorized_text['non_punc_message_body'].apply(lambda msg: tokenize_into_words(msg.lower()))\n",
    "    categorized_text['lematized_msg_words'] = categorized_text['tokenized_msg_body'].apply(lambda word_list: lemmatization(word_list))\n",
    "    categorized_text['lemmatized_msg_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
